{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cddecc-b33a-4b8a-aa98-959f5f74220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import utils\n",
    "import re\n",
    "import turicreate as tc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50a9c8-3bf8-41e0-8308-ab5318dbb95c",
   "metadata": {},
   "source": [
    "### 1) [2 marks] Imagine a movie production company wants to use a sentiment analysis model to identify positive/negative reviews of their movies. Which is worse for this use case, a false positive or a false negative, or are they equally bad? What value of β would be suitable for an Fβ score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68adebeb-2aec-47c7-9582-d05c3426275a",
   "metadata": {},
   "source": [
    "- Making the distinction between favorable and negative evaluations is crucial for decision-making in the film industry. When a production company invests in poorly reviewed films, it runs the danger of suffering financial loss and brand harm due to false positives, which mistakenly interpret negative reviews as positive. On the other hand, false negatives—positive evaluations that are mistakenly classified as negative—may lead to lost chances to profit from popular films. The specific film being analyzed and the goals of the company determine how serious each inaccuracy is. Achieving a balance between recall and precision is essential when choosing β for the Fβ score. While a lower β prioritizes recall and reduces false negatives, a higher β stresses precision and minimizes false positives. A more successful sentiment analysis strategy is ensured by tailoring this decision to the organization's priorities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bc841b-b88c-4d8a-a9d8-3d406eba40e7",
   "metadata": {},
   "source": [
    "### 2) [4 marks] Load the original dataset into a dataframe and use the regex Python library to clean the text data so that it is better suited for sentiment analysis. Add a markdown cell to explain what you are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a3ed0-367d-4ba0-8648-c065ee5bc70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tc.SFrame('IMDB_Dataset.csv')\n",
    "df = df[0:10] #keeping SFrame small for testing\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1659a-1f3c-4878-a383-4060dfa097f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of stopwords (this list is too short)\n",
    "#stopwords = ['a', 'an', 'and', \n",
    "#             'are', 'as', 'at', \n",
    "#             'be', 'but', 'by', \n",
    "#             'for', 'if', 'in', \n",
    "#             'into', 'is', 'it', \n",
    "#             'no', 'not', 'of', \n",
    "#             'on', 'or', 'such', \n",
    "#             'that', 'the', 'their', \n",
    "#             'then', 'there', 'these', \n",
    "#             'they', 'this', 'to', 'was', \n",
    "#             'will', 'with'\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14421f-ecfd-4a4b-8f51-20ca3d8e3b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of stopwords (this list is too short)\n",
    "stopwords = tc.text_analytics.stop_words()\n",
    "#stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03332ad0-378c-4dbd-b7e4-d94b77c630b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function cleans a single review.\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove html tags\n",
    "    text = re.sub('<.{1,4}>', '', text)\n",
    "    \n",
    "    # remove punctuation \n",
    "    text = re.sub('[^\\w^\\s\\n]', ' ', text)\n",
    "    \n",
    "    # remove numbers\n",
    "    text = re.sub(\"\\d\", \"\", text)\n",
    "    \n",
    "    # make everything lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove words with one to two characters\n",
    "    text = re.sub('\\\\b\\w{1,2}\\\\b', '', text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    for word in stopwords:\n",
    "        text = re.sub('\\\\b' + word + '\\\\b', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638b20d-9150-4b0d-8d03-0716264f34f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the clean_text() function to each row of the SFrame\n",
    "# note this is much faster than looping through row-by-row\n",
    "df['cleaned'] = df.apply(lambda x: clean_text(x['review']))\n",
    "#df['cleaned'][1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf128c5-6853-4a46-9e17-e1025d72bf11",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "The code performs text preprocessing on a dataset stored in an SFrame object. \n",
    "Here's a breakdown of what each section of the code does:\n",
    "\n",
    "1. **Loading Data:** Initially, it loads the data from a CSV file named 'IMDB_Dataset.csv' into an SFrame object named 'df'. To keep the SFrame small for testing purposes, only the first 10 rows are selected.\n",
    "\n",
    "2. **Defining Stopwords:** A list of stopwords is created. Stopwords are common words in a language (e.g., \"the\", \"is\", \"and\") that are typically filtered out before processing natural language data because they do not carry significant meaning.\n",
    "\n",
    "3. **Text Cleaning Function:** The 'clean_text()' function is defined to clean a single review. It performs several cleaning operations like removing HTML tags, punctuation, numbers, converting text to lowercase, removing short words, and removing stopwords.\n",
    "\n",
    "4. **Applying Text Cleaning:** The 'clean_text()' function is applied to each review in the 'review' column of the SFrame using the 'apply()' method. This process creates a new column named 'cleaned' in the SFrame, containing the cleaned text.\n",
    "\n",
    "5. **Counting Words:** The 'count_words()' function from the Turi Create library is used to count the frequency of each word in the 'cleaned' text. The word counts are stored in a new column named 'words' in the SFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1c08d-f420-4011-a706-7208fb278c4e",
   "metadata": {},
   "source": [
    "### 3) [1 mark] Load the cleaned data and labels into an SFrame. Add a column named ‘words’ to the SFrame that stores the count of each word used in each review. Print the SFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de72575-79a5-4f5c-997a-0d68a13b46a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting words and adding word-count dictionary to SFrame\n",
    "df['words'] = tc.text_analytics.count_words(df['cleaned'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab26005-5a98-42c3-82eb-d138c6331847",
   "metadata": {},
   "source": [
    "### 4) [1 mark] Split the data into training/validation/testing sets using 80%/10%/10% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e716d-3872-409c-aa58-10d833c41c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets using 80%/10%/10% respectively\n",
    "train_data, test_data = df.random_split(0.8)\n",
    "\n",
    "# Split the data into training and validation sets using a 50%/50% ratio\n",
    "train_data, validation_data = df.random_split(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a9cef-538f-4539-a3db-8e39a00f180e",
   "metadata": {},
   "source": [
    "### 5) [3 marks] Use Turicreate to create logistic classifiers for sentiment analysis. Experiment with different values of hyperparameters to develop two different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b3dfa-860c-44b5-bd35-d77a3b8caf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the first perceptron model with default hyperparameters\n",
    "model1 = tc.logistic_classifier.create(\n",
    "    train_data, target='sentiment', \n",
    "    features=['words'], \n",
    "    # Evaluate model performance on the validation set during training for hyperparameter tuning and preventing overfitting, and to estimate performance on unseen data.\n",
    "    validation_set = validation_data\n",
    ")\n",
    "\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546fd909-c132-4051-8710-f592b65b361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the second logistic classifier model with custom hyperparameters (e.g., different regularization parameters)\n",
    "model2 = tc.logistic_classifier.create(\n",
    "    train_data, \n",
    "    target='sentiment', \n",
    "    features=['words'], \n",
    "    step_size=0.8, \n",
    "    max_iterations=400,\n",
    "    # Evaluate model performance on the validation set during training for hyperparameter tuning and preventing overfitting, and to estimate performance on unseen data.\n",
    "    validation_set = validation_data\n",
    ")\n",
    "\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75c5ba-e929-40cb-8c6a-3aabe313de25",
   "metadata": {},
   "source": [
    "### 6) [4 marks] For each model:\n",
    "#### a) display the training and validation accuracies;\n",
    "#### b) display the confusion matrix on the validation set;\n",
    "#### c) calculate and display recall, precision, and Fβ score (using the value of β you chose above) on the validation set.\n",
    "#### d) plot the ROC curve and find the AUC for the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e6b374-03b6-4998-a3b2-160f8b683325",
   "metadata": {},
   "source": [
    "#### For Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068ee3a-2043-4bc3-8ae5-a25fdfba1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for training and validation data using Model 1\n",
    "class_predictions_train_model1 = model1.predict(train_data, output_type='class')\n",
    "class_predictions_val_model1 = model1.predict(validation_data, output_type='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb5dbd-ae11-46ef-a64e-db641d694822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training accuracy for Model 1\n",
    "train_accuracy_model1 = tc.evaluation.accuracy(train_data['sentiment'], class_predictions_train_model1)\n",
    "print(f\"Training Accuracy: {train_accuracy_model1}\")\n",
    "\n",
    "# Calculate validation accuracy for Model 1\n",
    "val_accuracy_model1 = tc.evaluation.accuracy(validation_data['sentiment'], class_predictions_val_model1)\n",
    "print(f\"Validation Accuracy: {val_accuracy_model1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ffaf3-dc58-4986-8e76-40470f20ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix for Model 1\n",
    "tc.evaluation.confusion_matrix(validation_data['sentiment'], class_predictions_val_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b3eb2-322a-48b5-8c17-610a01eb8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display Fβ score for Model 1\n",
    "tc.evaluation.fbeta_score(validation_data['sentiment'], class_predictions_val_model1, beta=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cbdf18-ce57-43a5-8576-620aa5c90c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities for Model 1\n",
    "probabilities_val_model1 = model1.predict(validation_data, output_type='probability')\n",
    "probabilities_val_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159c34e-3ec0-47fb-acc9-7e0e9e46039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_data_1 = tc.evaluation.roc_curve(validation_data['sentiment'], probabilities_val_model1)\n",
    "display(roc_data_1.head())\n",
    "display(roc_data_1.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f7bf3-29bc-4bc2-84d8-7e9b89f018e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_x_model1 = roc_data_1['tpr']\n",
    "roc_y_model1 = 1 - roc_data_1['fpr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c702c77-7772-4a42-b3c0-a52554c3f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(roc_x_model1, roc_y_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d7bd6-8c75-4cf0-ab62-53cae867d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc =  tc.evaluation.auc(validation_data['sentiment'], probabilities_val_model1)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad2d78b-9aa2-4920-bcfb-7def874cfafb",
   "metadata": {},
   "source": [
    "#### For Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f3c54a-c66d-41ff-8ae4-29308e13482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for training and validation data using Model 2\n",
    "class_predictions_train_model2 = model2.predict(train_data, output_type='class')\n",
    "class_predictions_val_model2 = model2.predict(validation_data, output_type='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d11920-2aeb-43a7-8316-eb1ce7411e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training accuracy for Model 2\n",
    "train_accuracy_model2 = tc.evaluation.accuracy(train_data['sentiment'], class_predictions_train_model2)\n",
    "print(f\"Training Accuracy: {train_accuracy_model2}\")\n",
    "\n",
    "# Calculate validation accuracy for Model 2\n",
    "val_accuracy_model2 = tc.evaluation.accuracy(validation_data['sentiment'], class_predictions_val_model2)\n",
    "print(f\"Validation Accuracy: {val_accuracy_model2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4757dc9-f4af-4fdf-858e-f35de3979c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix for Model 2\n",
    "tc.evaluation.confusion_matrix(validation_data['sentiment'], class_predictions_val_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ade88c-c66c-47cf-b424-b801aad1c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities for Model 2\n",
    "probabilities_val_model2 = model2.predict(validation_data, output_type='probability')\n",
    "probabilities_val_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff5275-639d-432a-a7df-40be90fb3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_data_2 =  tc.evaluation.roc_curve(validation_data['sentiment'], probabilities_val_model2)\n",
    "display(roc_data_2.head())\n",
    "display(roc_data_2.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95874-4246-403c-a23a-2757764382a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_x_model2 = roc_data_2['tpr']\n",
    "roc_y_model2 = 1 - roc_data_2['fpr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d3ce07-7543-47a4-b03c-01539b401197",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(roc_x_model2, roc_y_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ec87f-dd94-45de-af1e-6211200ba4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc =  tc.evaluation.auc(validation_data['sentiment'], probabilities_val_model2)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd0a9d-59b6-409a-8e9e-dc5a4074d33d",
   "metadata": {},
   "source": [
    "### 7) [1 mark] Select which of your two models is the best (or declare a tie) and justify your choice by commenting on metrics and the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c361a0-9c67-44ef-8647-71aa977ba988",
   "metadata": {},
   "source": [
    "- In terms of training accuracy, validation accuracy, and other evaluation metrics including precision, recall, and Fβ score, both Models 1 and 2 perform similarly. Additionally, there are no appreciable differences between the two models' confusion matrices in terms of their capacity to accurately identify instances and manage false positives and false negatives.\n",
    "\n",
    "- It is challenging to say with certainty one model is superior to the other because of the similarity in performance across different metrics and the lack of noticeable differences in the confusion matrices. Thus, we conclude that Models 1 and 2 are tied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64273f0b-7d37-433d-80ca-ab13fbd349a9",
   "metadata": {},
   "source": [
    "### 8) [2 marks] Using the test set:\n",
    "#### a) calculate and display the accuracy;\n",
    "#### b) display the confusion matrix;\n",
    "#### c) calculate and display recall, precision, and Fβ score.\n",
    "#### d) plot the ROC curve and find the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53759f-5fe0-4481-a870-6b438d041b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using Model 2 (my best choice of model)\n",
    "prediction_test_model = model2.predict(test_data, output_type='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255b1cc-9ed6-48a0-a1a5-7aeb71fa3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display accuracy on the test set\n",
    "tc.evaluation.accuracy(test_data['sentiment'], prediction_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2819b96e-6a56-42ca-83c7-d17243d0cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix for the test set\n",
    "tc.evaluation.confusion_matrix(test_data['sentiment'], prediction_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d9ec3-974e-46b9-a7db-d7b6c3b61542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display Fβ score for the test set\n",
    "tc.evaluation.fbeta_score(test_data['sentiment'], prediction_test_model, beta=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d004cb1-48f5-4da7-84f9-2650fa22879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities for Model 2\n",
    "probabilities_test = model2.predict(test_data, output_type='probability')\n",
    "probabilities_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f29c01-e33d-4680-9721-e4e7ebd6247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_test =  tc.evaluation.roc_curve(test_data['sentiment'], probabilities_test)\n",
    "display(roc_test.head())\n",
    "display(roc_test.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717f2b67-b4a2-4b20-9f2e-551863f4c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_x_test = roc_test['tpr']\n",
    "roc_y_test = 1 - roc_test['fpr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d0ffd4-be97-429f-ace0-4656cdfbd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(roc_x_test, roc_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea529b-5da9-4e7b-939c-0ec15206cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc =  tc.evaluation.auc(test_data['sentiment'], probabilities_test)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dcf972-5c71-47f4-9460-a44150b3edeb",
   "metadata": {},
   "source": [
    "# Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f2a9b-4a40-411d-b4d0-0652f3a11b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dbd62f-8f30-48d0-90f6-12fab6f7b3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
